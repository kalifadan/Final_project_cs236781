{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & General Settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from IPython.display import display\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import posixpath\n",
    "import wfdb\n",
    "import pycwt as wavelet\n",
    "from data import WaveletTransform, AFECGDataset\n",
    "from PIL import Image\n",
    "import dsp\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = unittest.TestCase()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'afdb'\n",
    "dataset = AFECGDataset(dataset_name, '../data/files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = dataset[0]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_per_sample = 20\n",
    "total_data_size = len(dataset)\n",
    "print(\"Total data size: \", total_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [dataset[i][0] for i in range(total_data_size)]\n",
    "# labels = [dataset[i][1] for i in range(total_data_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of one ECG sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples, label = data[0], labels[0]\n",
    "# print('P-signal: ', samples)\n",
    "# print('Has AF: ', 'Yes' if label == 1 else 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_wavelet = WaveletTransform(wavelet.Morlet(6), resample=20)\n",
    "# t = to_wavelet(data[0][0])\n",
    "# image_test = (t * 100 * 255).int() # Simple visualization test\n",
    "# transforms.ToPILImage()(image_test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Wavelet Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total data size is 1397\n",
    "# You can choose the data size \n",
    "# data_size = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fmt = '../data/new/sample_{}.pt'\n",
    "# to_wavelet = WaveletTransform(wavelet.Morlet(6), resample=20)\n",
    "# start = time.time()\n",
    "\n",
    "# sample_count = data_size\n",
    "# transformed_data = []\n",
    "# # transformed_labels= []\n",
    "\n",
    "# skip = 0\n",
    "# print('Preparing {} samples'.format(sample_count))\n",
    "# for sample_idx, sample in tqdm(enumerate(data[:sample_count])):\n",
    "#     wavelets = []\n",
    "#     filepath = fmt.format(sample_idx)\n",
    "    \n",
    "#     if os.path.isfile(filepath):\n",
    "#         # print('Skip {},{}'.format(sample_idx, signal_idx))\n",
    "#         t = torch.load(fmt.format(sample_idx, signal_idx))\n",
    "#         skip += 1\n",
    "#         transformed_data.append(t)\n",
    "#         continue\n",
    "#     # print(sample)\n",
    "#     for signal_idx, signal in enumerate(sample):\n",
    "#         # print(signal)\n",
    "#         # filepath = fmt.format(sample_idx, signal_idx)\n",
    "#         # if os.path.isfile(filepath):\n",
    "#              # print('Skip {},{}'.format(sample_idx, signal_idx))\n",
    "#         #     skip += 1\n",
    "#         #     continue\n",
    "#         wavelets.append(to_wavelet(signal))\n",
    "        \n",
    "#     t = torch.stack(wavelets)\n",
    "#     t = t.unsqueeze(1)\n",
    "#     transformed_data.append(t)\n",
    "    \n",
    "#     torch.save(t, filepath)\n",
    "    \n",
    "# end = time.time()\n",
    "# print('Elapsed time: {} ms'.format(1000 * (end - start)))\n",
    "# print('Skipped {} files'.format(skip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test set creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train, x_test, y_train, y_test =  train_test_split(transformed_data, labels[:sample_count], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def _calculate_ouput_size(padding, stride, kernel):\n",
    "        n_in = self.width * self.height\n",
    "        pass\n",
    "    \n",
    "    def __init__(self, size, in_channels=1):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.width, self.height = size\n",
    "                \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 10, kernel_size=(3,21)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(10, 10, kernel_size=(3,21)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2,2), stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(10, 10, kernel_size=(4,21)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(10, 10, kernel_size=(4,21)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(2540, 50)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        out = self.layer1(x)\n",
    "        # print(out.shape)\n",
    "        out = self.layer2(out)\n",
    "        # print(out.shape)\n",
    "        # out = out.reshape(out.size(0), -1)\n",
    "        out = self.layer3(out)\n",
    "        # print(out.shape)\n",
    "        out = self.layer4(out)\n",
    "        # print(out.shape)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        # print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ConvNet((256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0 = x_train[0][0].float()\n",
    "# encoder_cnn = ConvNet((375, 20))\n",
    "\n",
    "# display(x0.unsqueeze(0).shape)\n",
    "# h = encoder_cnn(x0.unsqueeze(0))\n",
    "# print(h.shape)\n",
    "\n",
    "# test.assertEqual(h.dim(), 2)\n",
    "# test.assertSequenceEqual(h.shape, (1, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model = encoder_cnn\n",
    "# num_epochs = 100\n",
    "# total_size = len(x_train)\n",
    "# test.assertEqual(total_size, len(y_train))\n",
    "\n",
    "# # Loss and optimizer\n",
    "# learning_rate = 0.001\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Train the model\n",
    "# total_step = len(x_train)\n",
    "# loss_list = []\n",
    "# acc_list = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     acc = 0\n",
    "#     for idx, (samples, label) in enumerate(zip(x_train, y_train)):\n",
    "#         label = torch.tensor([label]).long()\n",
    "#         for image in samples:\n",
    "            \n",
    "#             # Run the forward pass\n",
    "#             output = model(image.unsqueeze(0).float())\n",
    "#             loss = criterion(output, label)\n",
    "#             loss_list.append(loss.item())\n",
    "            \n",
    "#             # Backprop and perform Adam optimisation\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # Track the accuracy\n",
    "#             _, predicted = torch.max(output.data, 1)\n",
    "#             correct = (predicted == label).sum().item()\n",
    "#             acc += correct\n",
    "#             print(correct)\n",
    "                        \n",
    "#     acc = acc / (total_size * 20)          \n",
    "#     acc_list.append(acc)\n",
    "#     print('Epoch [{}/{}], Accuracy: {:.2f}%'\n",
    "#           .format(epoch + 1, num_epochs, acc * 100))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_gates):\n",
    "        super(BRNN, self).__init__()\n",
    "        self.bi_rnn = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_gates,\n",
    "                                    batch_first=False, bidirectional=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        output, hn = self.bi_rnn(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(BRNN(50, 50, images_per_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notations:\n",
    "\n",
    "* $Y = \\left[ y_1, \\ldots, y_T \\right]$ – the input matrix of size $\\left( N \\times T \\right)$, where $N$ is the number of features in a single output vector of the BRNN\n",
    "\n",
    "* $w_\\mathrm{att}$ – The parameters of the attention model, of size $\\left( N \\times 1 \\right)$, where $N$ is the number of features in a single output vector of the BRNN\n",
    "\n",
    "* $\\alpha$ – The attention weights, given as $\\alpha = \\mathrm{softmax} \\left( w_\\mathrm{att}^T Y \\right)$. This is an element-wise softmax, where the output size of $\\alpha$ is $\\left( 1 \\times T \\right)$\n",
    "\n",
    "* $h_\\mathrm{att}$ – Output of the attention mechanism, given by $h_\\mathrm{att} = Y \\alpha^T$, of size $\\left( N \\times 1 \\right)$, i.e. a vector of $N$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxAttention(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SoftmaxAttention, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(1, input_size))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # X [T, B, N]\n",
    "        # weight [N x 1]\n",
    "        batch_size = X.size(1)\n",
    "        X = X.transpose(0,1) # (B, T, N)\n",
    "        alignment_scores = X.matmul(self.weight.t())\n",
    "        # print('AS: ', alignment_scores)\n",
    "        # print('wT: ', self.weight.t().shape)\n",
    "        # print('AS: ', alignment_scores.shape)\n",
    "        # alpha [T x 1]\n",
    "        attn_weights = nn.functional.softmax(alignment_scores, dim=0)\n",
    "        # print('X: ', X.shape)\n",
    "        # print('ATT: ', attn_weights.shape)\n",
    "        # print('X: ', X.shape)\n",
    "        # h_att [1 x N]\n",
    "        return torch.bmm(attn_weights.transpose(1, 2), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Baseline, self).__init__()\n",
    "        \n",
    "        lst = []\n",
    "        for i in range(images_per_sample):\n",
    "            conv = ConvNet((375, 20))\n",
    "            lst.append(conv)\n",
    "            self.add_module('conv{}'.format(i), conv)\n",
    "            \n",
    "        self.cnn_layers = lst\n",
    "        self.brnn = BRNN(50, 50, images_per_sample)\n",
    "        self.attention = SoftmaxAttention(100)\n",
    "        self.fc = nn.Linear(100, 2)\n",
    "\n",
    "\n",
    "    def forward(self, X): \n",
    "        \"\"\"\n",
    "        Perform forward propagation on a batch of data\n",
    "        :param X: A tensor of shape (B, N, W, H) where (W, H) are the image dimensions,\n",
    "        N is the number of images per sample, and B is the batch size\n",
    "        \"\"\"\n",
    "        X = X / X.min()\n",
    "        print('input: ', X.shape)\n",
    "        # print(X[0,0,:].float().unsqueeze(1), X[0,1,:].float().unsqueeze(1))\n",
    "        out = [self.cnn_layers[i](X[:,i,:].float().unsqueeze(1)) for i in range(images_per_sample)]\n",
    "        out = torch.stack(out)\n",
    "        # print('Equal:', out[0][0] == out[0][1])\n",
    "        # print('After CNN: ', out[0][0], out[0][1])\n",
    "        out = self.brnn(out)\n",
    "        print('After BRNN: ', out)\n",
    "        out = self.attention(out.squeeze(1))\n",
    "        out = out.squeeze(1)\n",
    "        print('After ATT: ', out)\n",
    "        out = self.fc(out)\n",
    "        out = out.squeeze(1)\n",
    "        print('After FC: ', out)\n",
    "        return F.softmax(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display(Baseline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, label = dataset[0]\n",
    "# label.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Baseline()\n",
    "num_epochs = 200\n",
    "# total_size = len(x_train)\n",
    "# test.assertEqual(total_size, len(y_train))\n",
    "\n",
    "# Preparation\n",
    "dataloader = DataLoader(dataset, batch_size=90)\n",
    "\n",
    "# Loss and optimizer\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Train the model\n",
    "total_step = total_data_size\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    acc = 0\n",
    "    total_size = len(dataset)\n",
    "    for batch_data, batch_labels in tqdm(dataloader):\n",
    "\n",
    "        print(batch_data.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Run the forward pass\n",
    "        output = model(batch_data)\n",
    "        \n",
    "        print(output.shape, batch_labels.shape)\n",
    "        \n",
    "        loss = criterion(output, batch_labels)\n",
    "        \n",
    "        # Backprop and perform optimisation\n",
    "        loss_list.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Track the accuracy\n",
    "#         probability = torch.distributions.categorical.Categorical(output)\n",
    "#         prediction = probability.sample()\n",
    "        prediction = output.argmax(dim=1)\n",
    "        \n",
    "        print('Output:', output)\n",
    "        print('Ground truth:', batch_labels)\n",
    "        print('Prediction:', prediction)\n",
    "        correct = (prediction == batch_labels).sum().item()\n",
    "        print('Correct: {}'.format(correct))\n",
    "        acc += correct\n",
    "                    \n",
    "    acc = acc / (total_size)          \n",
    "    acc_list.append(acc)\n",
    "    print('Epoch [{}/{}], Accuracy: {:.2f}%'\n",
    "          .format(epoch + 1, num_epochs, acc * 100))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
