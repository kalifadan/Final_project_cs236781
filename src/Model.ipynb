{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & General Settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "import matplotlib as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# Our imports\n",
    "from data import WaveletTransform, AFECGDataset\n",
    "import dsp\n",
    "from model.blocks import ConvNet, BRNN, SoftmaxAttention\n",
    "from model.baseline import Baseline\n",
    "from training import train\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "test = unittest.TestCase()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'afdb'\n",
    "dataset = AFECGDataset(dataset_name, '../data/files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 249.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing 1397 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1397it [00:04, 336.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 4165.770053863525 ms\n",
      "Skipped 1397 files which had a backup\n"
     ]
    }
   ],
   "source": [
    "dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 20, 375])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, label = dataset[0]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data size:  1397\n"
     ]
    }
   ],
   "source": [
    "images_per_sample = 20\n",
    "total_data_size = len(dataset)\n",
    "print(\"Total data size: \", total_data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [dataset[i][0] for i in range(total_data_size)]\n",
    "# labels = [dataset[i][1] for i in range(total_data_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of one ECG sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples, label = data[0], labels[0]\n",
    "# print('P-signal: ', samples)\n",
    "# print('Has AF: ', 'Yes' if label == 1 else 'No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_wavelet = WaveletTransform(wavelet.Morlet(6), resample=20)\n",
    "# t = to_wavelet(data[0][0])\n",
    "# image_test = (t * 100 * 255).int() # Simple visualization test\n",
    "# transforms.ToPILImage()(image_test).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Wavelet Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total data size is 1397\n",
    "# You can choose the data size \n",
    "data_size = len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test set creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 10, kernel_size=(3, 21), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(3, 21), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(4, 21), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Conv2d(10, 10, kernel_size=(4, 21), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc): Linear(in_features=2540, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ConvNet((256, 256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0 = x_train[0][0].float()\n",
    "# encoder_cnn = ConvNet((375, 20))\n",
    "\n",
    "# display(x0.unsqueeze(0).shape)\n",
    "# h = encoder_cnn(x0.unsqueeze(0))\n",
    "# print(h.shape)\n",
    "\n",
    "# test.assertEqual(h.dim(), 2)\n",
    "# test.assertSequenceEqual(h.shape, (1, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BRNN(\n",
       "  (bi_rnn): RNN(50, 50, num_layers=20, bidirectional=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(BRNN(50, 50, images_per_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notations:\n",
    "\n",
    "* $Y = \\left[ y_1, \\ldots, y_T \\right]$ – the input matrix of size $\\left( N \\times T \\right)$, where $N$ is the number of features in a single output vector of the BRNN\n",
    "\n",
    "* $w_\\mathrm{att}$ – The parameters of the attention model, of size $\\left( N \\times 1 \\right)$, where $N$ is the number of features in a single output vector of the BRNN\n",
    "\n",
    "* $\\alpha$ – The attention weights, given as $\\alpha = \\mathrm{softmax} \\left( w_\\mathrm{att}^T Y \\right)$. This is an element-wise softmax, where the output size of $\\alpha$ is $\\left( 1 \\times T \\right)$\n",
    "\n",
    "* $h_\\mathrm{att}$ – Output of the attention mechanism, given by $h_\\mathrm{att} = Y \\alpha^T$, of size $\\left( N \\times 1 \\right)$, i.e. a vector of $N$ features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SoftmaxAttention()"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SoftmaxAttention(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 20, 375])\n",
      "After FC:  tensor([[-6.1794,  8.4013],\n",
      "        [-6.1790,  8.4008]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "data1, label1 = dataset[0]\n",
    "data2, label2 = dataset[1]\n",
    "batch_data = torch.cat([data1.unsqueeze(0) * 10, data2.unsqueeze(0) * 10], dim=0)\n",
    "print(batch_data.shape)\n",
    "output = model(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7fae36d546d473a89c3d010054403a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=200.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4060f4c023824b61a742fac0a60af888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After FC:  tensor([[ 0.1204, -0.0548],\n",
      "        [ 0.1146, -0.0491],\n",
      "        [ 0.1159, -0.0518],\n",
      "        [ 0.1210, -0.0560],\n",
      "        [ 0.1198, -0.0543],\n",
      "        [ 0.1210, -0.0531],\n",
      "        [ 0.1186, -0.0534],\n",
      "        [ 0.1128, -0.0527],\n",
      "        [ 0.1147, -0.0526],\n",
      "        [ 0.1184, -0.0510]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.5437, 0.4563],\n",
      "        [0.5408, 0.4592],\n",
      "        [0.5418, 0.4582],\n",
      "        [0.5441, 0.4559],\n",
      "        [0.5434, 0.4566],\n",
      "        [0.5434, 0.4566],\n",
      "        [0.5429, 0.4571],\n",
      "        [0.5413, 0.4587],\n",
      "        [0.5417, 0.4583],\n",
      "        [0.5423, 0.4577]], grad_fn=<SliceBackward>)\n",
      "Labels: tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1])\n",
      "Output: tensor([[0.5437, 0.4563],\n",
      "        [0.5408, 0.4592],\n",
      "        [0.5418, 0.4582],\n",
      "        [0.5441, 0.4559],\n",
      "        [0.5434, 0.4566],\n",
      "        [0.5434, 0.4566],\n",
      "        [0.5429, 0.4571],\n",
      "        [0.5413, 0.4587],\n",
      "        [0.5417, 0.4583],\n",
      "        [0.5423, 0.4577],\n",
      "        [0.5421, 0.4579],\n",
      "        [0.5426, 0.4574],\n",
      "        [0.5411, 0.4589],\n",
      "        [0.5416, 0.4584],\n",
      "        [0.5403, 0.4597],\n",
      "        [0.5429, 0.4571],\n",
      "        [0.5421, 0.4579],\n",
      "        [0.5421, 0.4579],\n",
      "        [0.5414, 0.4586],\n",
      "        [0.5428, 0.4572],\n",
      "        [0.5413, 0.4587],\n",
      "        [0.5421, 0.4579],\n",
      "        [0.5433, 0.4567],\n",
      "        [0.5418, 0.4582],\n",
      "        [0.5424, 0.4576],\n",
      "        [0.5439, 0.4561],\n",
      "        [0.5418, 0.4582],\n",
      "        [0.5422, 0.4578],\n",
      "        [0.5428, 0.4572],\n",
      "        [0.5417, 0.4583],\n",
      "        [0.5410, 0.4590],\n",
      "        [0.5432, 0.4568],\n",
      "        [0.5428, 0.4572],\n",
      "        [0.5414, 0.4586],\n",
      "        [0.5412, 0.4588],\n",
      "        [0.5424, 0.4576],\n",
      "        [0.5418, 0.4582],\n",
      "        [0.5427, 0.4573],\n",
      "        [0.5426, 0.4574],\n",
      "        [0.5429, 0.4571],\n",
      "        [0.5409, 0.4591],\n",
      "        [0.5417, 0.4583],\n",
      "        [0.5409, 0.4591],\n",
      "        [0.5420, 0.4580],\n",
      "        [0.5430, 0.4570],\n",
      "        [0.5424, 0.4576],\n",
      "        [0.5443, 0.4557],\n",
      "        [0.5426, 0.4574],\n",
      "        [0.5433, 0.4567],\n",
      "        [0.5431, 0.4569],\n",
      "        [0.5427, 0.4573],\n",
      "        [0.5434, 0.4566],\n",
      "        [0.5413, 0.4587],\n",
      "        [0.5428, 0.4572],\n",
      "        [0.5421, 0.4579],\n",
      "        [0.5435, 0.4565],\n",
      "        [0.5424, 0.4576],\n",
      "        [0.5407, 0.4593],\n",
      "        [0.5413, 0.4587],\n",
      "        [0.5419, 0.4581],\n",
      "        [0.5416, 0.4584],\n",
      "        [0.5416, 0.4584],\n",
      "        [0.5421, 0.4579],\n",
      "        [0.5441, 0.4559],\n",
      "        [0.5436, 0.4564],\n",
      "        [0.5415, 0.4585],\n",
      "        [0.5422, 0.4578],\n",
      "        [0.5423, 0.4577],\n",
      "        [0.5429, 0.4571],\n",
      "        [0.5417, 0.4583],\n",
      "        [0.5434, 0.4566],\n",
      "        [0.5421, 0.4579],\n",
      "        [0.5402, 0.4598],\n",
      "        [0.5429, 0.4571],\n",
      "        [0.5435, 0.4565],\n",
      "        [0.5433, 0.4567],\n",
      "        [0.5427, 0.4573],\n",
      "        [0.5426, 0.4574],\n",
      "        [0.5437, 0.4563],\n",
      "        [0.5436, 0.4564],\n",
      "        [0.5415, 0.4585],\n",
      "        [0.5426, 0.4574],\n",
      "        [0.5435, 0.4565],\n",
      "        [0.5423, 0.4577],\n",
      "        [0.5436, 0.4564],\n",
      "        [0.5437, 0.4563],\n",
      "        [0.5423, 0.4577],\n",
      "        [0.5425, 0.4575],\n",
      "        [0.5426, 0.4574],\n",
      "        [0.5425, 0.4575]], grad_fn=<SoftmaxBackward>)\n",
      "Ground truth: tensor([1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1])\n",
      "Prediction: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Correct: 77\n",
      "After FC:  tensor([[ 0.8543, -0.7321],\n",
      "        [ 0.8543, -0.7322],\n",
      "        [ 0.8539, -0.7318],\n",
      "        [ 0.8537, -0.7316],\n",
      "        [ 0.8540, -0.7319],\n",
      "        [ 0.8536, -0.7316],\n",
      "        [ 0.8543, -0.7322],\n",
      "        [ 0.8537, -0.7317],\n",
      "        [ 0.8542, -0.7320],\n",
      "        [ 0.8541, -0.7320]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8299, 0.1701],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699]], grad_fn=<SliceBackward>)\n",
      "Labels: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "Output: tensor([[0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8299, 0.1701],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8302, 0.1698],\n",
      "        [0.8299, 0.1701],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8299, 0.1701],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8299, 0.1701],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8299, 0.1701],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8299, 0.1701],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8302, 0.1698],\n",
      "        [0.8299, 0.1701],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8299, 0.1701],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8302, 0.1698],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.8300, 0.1700],\n",
      "        [0.8299, 0.1701],\n",
      "        [0.8300, 0.1700]], grad_fn=<SoftmaxBackward>)\n",
      "Ground truth: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])\n",
      "Prediction: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Correct: 53\n",
      "After FC:  tensor([[ 0.2258, -0.1440],\n",
      "        [ 0.2259, -0.1441],\n",
      "        [ 0.2260, -0.1441],\n",
      "        [ 0.2257, -0.1439],\n",
      "        [ 0.2260, -0.1441],\n",
      "        [ 0.2258, -0.1441],\n",
      "        [ 0.2258, -0.1439],\n",
      "        [ 0.2256, -0.1438],\n",
      "        [ 0.2257, -0.1439],\n",
      "        [ 0.2258, -0.1440]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5913, 0.4087],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5913, 0.4087],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086]], grad_fn=<SliceBackward>)\n",
      "Labels: tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5913, 0.4087],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5913, 0.4087],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5913, 0.4087],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5913, 0.4087],\n",
      "        [0.5913, 0.4087],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5913, 0.4087],\n",
      "        [0.5913, 0.4087],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5913, 0.4087],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5913, 0.4087],\n",
      "        [0.5915, 0.4085],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5914, 0.4086],\n",
      "        [0.5915, 0.4085]], grad_fn=<SoftmaxBackward>)\n",
      "Ground truth: tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Prediction: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Correct: 82\n",
      "After FC:  tensor([[0.1378, 0.0342],\n",
      "        [0.1372, 0.0347],\n",
      "        [0.1380, 0.0339],\n",
      "        [0.1378, 0.0342],\n",
      "        [0.1379, 0.0341],\n",
      "        [0.1374, 0.0346],\n",
      "        [0.1372, 0.0347],\n",
      "        [0.1384, 0.0335],\n",
      "        [0.1380, 0.0339],\n",
      "        [0.1381, 0.0339]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.5259, 0.4741],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5260, 0.4740],\n",
      "        [0.5259, 0.4741],\n",
      "        [0.5259, 0.4741],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5262, 0.4738],\n",
      "        [0.5260, 0.4740],\n",
      "        [0.5260, 0.4740]], grad_fn=<SliceBackward>)\n",
      "Labels: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Output: tensor([[0.5259, 0.4741],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5260, 0.4740],\n",
      "        [0.5259, 0.4741],\n",
      "        [0.5259, 0.4741],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5262, 0.4738],\n",
      "        [0.5260, 0.4740],\n",
      "        [0.5260, 0.4740],\n",
      "        [0.5258, 0.4742],\n",
      "        [0.5254, 0.4746],\n",
      "        [0.5255, 0.4745],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5255, 0.4745],\n",
      "        [0.5258, 0.4742],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.5259, 0.4741],\n",
      "        [0.5258, 0.4742],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.5254, 0.4746],\n",
      "        [0.5255, 0.4745],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5255, 0.4745],\n",
      "        [0.5253, 0.4747],\n",
      "        [0.5250, 0.4750],\n",
      "        [0.5253, 0.4747],\n",
      "        [0.5254, 0.4746],\n",
      "        [0.5252, 0.4748],\n",
      "        [0.5252, 0.4748],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5252, 0.4748],\n",
      "        [0.5254, 0.4746],\n",
      "        [0.5255, 0.4745],\n",
      "        [0.5258, 0.4742],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.5255, 0.4745],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.5255, 0.4745],\n",
      "        [0.5259, 0.4741],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.5258, 0.4742],\n",
      "        [0.5260, 0.4740],\n",
      "        [0.5263, 0.4737],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.5259, 0.4741],\n",
      "        [0.5259, 0.4741],\n",
      "        [0.5263, 0.4737],\n",
      "        [0.5259, 0.4741],\n",
      "        [0.5257, 0.4743],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5253, 0.4747],\n",
      "        [0.5260, 0.4740],\n",
      "        [0.5260, 0.4740],\n",
      "        [0.5255, 0.4745],\n",
      "        [0.5259, 0.4741],\n",
      "        [0.5258, 0.4742],\n",
      "        [0.5254, 0.4746],\n",
      "        [0.5260, 0.4740],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5255, 0.4745],\n",
      "        [0.5258, 0.4742],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5252, 0.4748],\n",
      "        [0.5258, 0.4742],\n",
      "        [0.5258, 0.4742],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5255, 0.4745],\n",
      "        [0.5259, 0.4741],\n",
      "        [0.5259, 0.4741],\n",
      "        [0.5260, 0.4740],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5255, 0.4745],\n",
      "        [0.5258, 0.4742],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5256, 0.4744],\n",
      "        [0.5254, 0.4746],\n",
      "        [0.5253, 0.4747],\n",
      "        [0.5252, 0.4748],\n",
      "        [0.5255, 0.4745],\n",
      "        [0.5258, 0.4742],\n",
      "        [0.5253, 0.4747],\n",
      "        [0.5259, 0.4741]], grad_fn=<SoftmaxBackward>)\n",
      "Ground truth: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Prediction: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Correct: 84\n",
      "After FC:  tensor([[ 0.8991, -1.0339],\n",
      "        [ 0.8991, -1.0339],\n",
      "        [ 0.8991, -1.0339],\n",
      "        [ 0.8991, -1.0339],\n",
      "        [ 0.8991, -1.0339],\n",
      "        [ 0.8991, -1.0339],\n",
      "        [ 0.8991, -1.0339],\n",
      "        [ 0.8991, -1.0339],\n",
      "        [ 0.8991, -1.0339],\n",
      "        [ 0.8991, -1.0339]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264]], grad_fn=<SliceBackward>)\n",
      "Labels: tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0])\n",
      "Output: tensor([[0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264],\n",
      "        [0.8736, 0.1264]], grad_fn=<SoftmaxBackward>)\n",
      "Ground truth: tensor([0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0])\n",
      "Prediction: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Correct: 63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After FC:  tensor([[ 0.3657, -0.5715],\n",
      "        [ 0.3657, -0.5715],\n",
      "        [ 0.3657, -0.5715],\n",
      "        [ 0.3657, -0.5715],\n",
      "        [ 0.3657, -0.5715],\n",
      "        [ 0.3657, -0.5715],\n",
      "        [ 0.3657, -0.5715],\n",
      "        [ 0.3657, -0.5715],\n",
      "        [ 0.3657, -0.5715],\n",
      "        [ 0.3657, -0.5715]], grad_fn=<SliceBackward>)\n",
      "tensor([[0.7185, 0.2815],\n",
      "        [0.7185, 0.2815],\n",
      "        [0.7185, 0.2815],\n",
      "        [0.7185, 0.2815],\n",
      "        [0.7185, 0.2815],\n",
      "        [0.7185, 0.2815],\n",
      "        [0.7185, 0.2815],\n",
      "        [0.7185, 0.2815],\n",
      "        [0.7185, 0.2815],\n",
      "        [0.7185, 0.2815]], grad_fn=<SliceBackward>)\n",
      "Labels: tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/miki/opt/miniconda3/envs/cs236781-project/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-8025f910e5ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/final_project_cs236781/src/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, config)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Backprop and perform optimisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mloss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/cs236781-project/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/cs236781-project/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Baseline()\n",
    "config = dict(\n",
    "    num_workers=4,\n",
    "    batch_size=90,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.01,\n",
    "    num_epochs=200,\n",
    "    is_notebook=True\n",
    ")\n",
    "\n",
    "train(model, dataset, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
